
@article{8961101,
  title = {Theoretical and Experimental Analysis of a 4 × 4 Reconfigurable {{MZI-Based}} Linear Optical Processor},
  author = {Shokraneh, Farhad and Nezami, Mohammadreza Sanadgol and Liboiron-Ladouceur, Odile},
  date = {2020},
  journaltitle = {Journal of Lightwave Technology},
  volume = {38},
  number = {6},
  pages = {1258--1267},
  doi = {10.1109/JLT.2020.2966949}
}

@misc{chowdhery2022palm,
  title = {{{PaLM}}: {{Scaling}} Language Modeling with Pathways},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  date = {2022},
  eprint = {2204.02311},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv}
}

@article{delimaMachineLearningNeuromorphic2019,
  title = {Machine Learning with Neuromorphic Photonics},
  author = {De Lima, Thomas Ferreira and Peng, Hsuan-Tung and Tait, Alexander N. and Nahmias, Mitchell A. and Miller, Heidi B. and Shastri, Bhavin J. and Prucnal, Paul R.},
  date = {2019},
  journaltitle = {Journal of Lightwave Technology},
  volume = {37},
  number = {5},
  pages = {1515--1534},
  publisher = {{IEEE}},
  file = {/Users/dhushyanth/Zotero/storage/W2TBZEN3/8662590.html}
}

@article{demarinisCodesignedIntegratedPhotonic2022,
  title = {A Codesigned Integrated Photonic Electronic Neuron},
  author = {De Marinis, Lorenzo and Catania, Alessandro and Castoldi, Piero and Contestabile, Giampiero and Bruschi, Paolo and Piotto, Massimo and Andriolli, Nicola},
  date = {2022},
  journaltitle = {IEEE Journal of Quantum Electronics},
  publisher = {{IEEE}},
  file = {/Users/dhushyanth/Zotero/storage/FH6LFVW2/9781309.html}
}

@article{giordanoAnalogtoDigitalConversionReconfigurable2019,
  title = {Analog-to-{{Digital Conversion With Reconfigurable Function Mapping}} for {{Neural Networks Activation Function Acceleration}}},
  author = {Giordano, Massimo and Cristiano, Giorgio and Ishibashi, Koji and Ambrogio, Stefano and Tsai, Hsinyu and Burr, Geoffrey W. and Narayanan, Pritish},
  date = {2019-06},
  journaltitle = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume = {9},
  number = {2},
  pages = {367--376},
  issn = {2156-3365},
  doi = {10.1109/JETCAS.2019.2911537},
  abstract = {Hardware acceleration of deep neural networks (DNNs) using non-volatile memory arrays has the potential to achieve orders of magnitude power and performance benefits versus digital von-Neumann architectures by implementing the critical multiply-accumulate operations at the location of the weight data. However, realizing these system-level improvements requires careful consideration of the circuit design tradeoffs involved. For instance, neuron circuitry at the periphery, in addition to accumulating current and having mechanisms for routing, must also implement a non-linear activation function (for forward propagate) or a derivative (for reverse propagate). While it is possible to do this with analog-to-digital converters (ADCs) followed by digital arithmetic circuitry, this approach is powerhungry, suffers from undersampling, and could occupy a large area footprint. These large circuit blocks may therefore need to be time-multiplexed across multiple neurons, reducing the overall parallelism and diminishing the performance benefits. In this paper, we propose a new function mapping ADC that directly implements non-linear functions as a part of the process of conversion into the digital domain. The design is applicable to both inference and training, since it is capable of implementing both the activation function and its derivative using the same hardware. It is capable of fast and parallel conversion across all neuron values, while also being flexible and reconfigurable. We describe the design, followed by detailed circuit-level simulations demonstrating the viability and flexibility of the approach and quantifying the power and performance numbers. The simulation results show a total conversion time of 207 ns for 512 neurons in parallel, while the total energy consumption is found to be 9.95 nJ, which corresponds to 19.4 pJ per neuron.},
  eventtitle = {{{IEEE Journal}} on {{Emerging}} and {{Selected Topics}} in {{Circuits}} and {{Systems}}},
  keywords = {activation functions,analog accelerator,analog integrated circuits,Analog-digital conversion,analog-to-digital conversion,Arrays,Biological neural networks,Deep learning,functional analog-to-digital converter (ADC),Neurons,non-volatile memory (NVM),Nonvolatile memory,Quantization (signal),Training},
  file = {/Users/dhushyanth/Zotero/storage/3UDPZSBQ/Giordano et al. - 2019 - Analog-to-Digital Conversion With Reconfigurable F.pdf;/Users/dhushyanth/Zotero/storage/PNUBMJNR/8692407.html}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  publisher = {{MIT Press}}
}

@online{NVIDIAT4Tensor,
  title = {{{NVIDIA T4 Tensor Core GPU}} for {{AI Inference}} | {{NVIDIA Data Center}}},
  url = {https://www.nvidia.com/en-us/data-center/tesla-t4/},
  urldate = {2023-05-08}
}

@article{photonics9020120,
  title = {An All-{{MRR-based}} Photonic Spiking Neural Network for Spike Sequence Learning},
  author = {Han, Yanan and Xiang, Shuiying and Zhang, Yuna and Gao, Shuang and Wen, Aijun and Hao, Yue},
  date = {2022},
  journaltitle = {Photonics},
  volume = {9},
  number = {2},
  issn = {2304-6732},
  doi = {10.3390/photonics9020120},
  url = {https://www.mdpi.com/2304-6732/9/2/120},
  abstract = {Photonic spiking neural networks (SNN) have the advantages of high power efficiency, high bandwidth and low delay, but limitations are encountered in large-scale integration. The silicon photonics platform is a promising candidate for realizing large-scale photonic SNN because it is compatible with the current mature CMOS platforms. Here, we present an architecture of photonic SNN which consists of photonic neuron, photonic spike timing dependent plasticity (STDP) and weight configuration that are all based on silicon micro-ring resonators (MRRs), via taking advantage of the nonlinear effects in silicon. The photonic spiking neuron based on the add-drop MRR is proposed, and a system-level computational model of all-MRR-based photonic SNN is presented. The proposed architecture could exploit the properties of small area, high integration and flexible structure of MRR, but also faces challenges caused by the high sensitivity of MRR. The spike sequence learning problem is addressed based on the proposed all-MRR-based photonic SNN architecture via adopting supervised training algorithms. We show the importance of algorithms when hardware devices are limited.},
  article-number = {120}
}

@article{tanakaUltralowPower59MW2018,
  title = {Ultralow-{{Power}} (1.59 {{mW}}/{{Gbps}}), 56-{{Gbps PAM4 Operation}} of {{Si Photonic Transmitter Integrating Segmented PIN Mach}}–{{Zehnder Modulator}} and 28-Nm {{CMOS Driver}}},
  author = {Tanaka, Shinsuke and Simoyama, Takasi and Aoki, Tsuyoshi and Mori, Toshihiko and Sekiguchi, Shigeaki and Jeong, Seok-Hwan and Usuki, Tatsuya and Tanaka, Yu and Morito, Ken},
  date = {2018-03},
  journaltitle = {Journal of Lightwave Technology},
  volume = {36},
  number = {5},
  pages = {1275--1280},
  issn = {1558-2213},
  doi = {10.1109/JLT.2018.2799965},
  abstract = {A highly power-efficient silicon (Si) photonic PAM4 transmitter was developed by integrating a Si segmented Mach- Zehnder modulator and a CMOS driver chip. Si p-i-n-type phase shifters are directly driven with a CMOS inverter driver array to realize a low power operation. A passive RC equalizing technique was adopted to extend the modulation bandwidth up to 20 GHz while maintaining a low power consumption. By integrating a passive RC filter within the photonics chip, we achieved a very compact foot print for the transmitter (450 × 950 μm). The fabricated modulator exhibited a low VπL of 0.19 V·cm and a moderate insertion loss of 23.7 dB/cm . The transmitter successfully demonstrated clear eye openings of PAM4 signal up to 56 Gbps together with a record-high-efficiency of 1.59 mW/Gbps. A low bit-errorrate below KP4 FEC limit ({$<$};2.0 × 10-4) was also confirmed at 50-Gbps PAM4 operation even with an unequalized receiver.},
  eventtitle = {Journal of {{Lightwave Technology}}},
  keywords = {Bandwidth,CMOS driver,Inverters,Modulation,Optical filters,optical modulators,Optical transmitters,Photonics,Silicon,silicon photonics},
  file = {/Users/dhushyanth/Zotero/storage/7YYVBQDA/Tanaka et al. - 2018 - Ultralow-Power (1.59 mWGbps), 56-Gbps PAM4 Operat.pdf;/Users/dhushyanth/Zotero/storage/5Q9NZFIB/8276231.html}
}


